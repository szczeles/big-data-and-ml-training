{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "spark = SparkSession.builder.enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|    pokes|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE `pokes`(`foo` INT, `bar` STRING)\n",
      "ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'\n",
      "WITH SERDEPROPERTIES (\n",
      "  'serialization.format' = '1'\n",
      ")\n",
      "STORED AS\n",
      "  INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat'\n",
      "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
      "TBLPROPERTIES (\n",
      "  'transient_lastDdlTime' = '1569241824'\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql('show create table pokes').first().createtab_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                 |comment|\n",
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "|foo                         |int                                                       |null   |\n",
      "|bar                         |string                                                    |null   |\n",
      "|                            |                                                          |       |\n",
      "|# Detailed Table Information|                                                          |       |\n",
      "|Database                    |default                                                   |       |\n",
      "|Table                       |pokes                                                     |       |\n",
      "|Owner                       |root                                                      |       |\n",
      "|Created Time                |Mon Sep 23 12:29:50 UTC 2019                              |       |\n",
      "|Last Access                 |Thu Jan 01 00:00:00 UTC 1970                              |       |\n",
      "|Created By                  |Spark 2.2 or prior                                        |       |\n",
      "|Type                        |MANAGED                                                   |       |\n",
      "|Provider                    |hive                                                      |       |\n",
      "|Table Properties            |[transient_lastDdlTime=1569241824]                        |       |\n",
      "|Statistics                  |5812 bytes                                                |       |\n",
      "|Location                    |hdfs://namenode:9000/user/hive/warehouse/pokes            |       |\n",
      "|Serde Library               |org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe        |       |\n",
      "|InputFormat                 |org.apache.hadoop.mapred.TextInputFormat                  |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat|       |\n",
      "|Storage Properties          |[serialization.format=1]                                  |       |\n",
      "|Partition Provider          |Catalog                                                   |       |\n",
      "+----------------------------+----------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('describe formatted pokes').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 814 µs, sys: 32 µs, total: 846 µs\n",
      "Wall time: 150 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "spark.table('pokes').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, UserId: int, Name: string, Date: timestamp, Class: int, TagBased: boolean]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.createTable('badges', path='/data/stackoverflow/Badges')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|   badges|      false|\n",
      "| default|    pokes|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(10).write.saveAsTable('numbers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Various formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael, 29\r\n",
      "Andy, 30\r\n",
      "Justin, 19\r\n"
     ]
    }
   ],
   "source": [
    "! cat /usr/local/spark/examples/src/main/resources/people.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|      value|\n",
      "+-----------+\n",
      "|Michael, 29|\n",
      "|   Andy, 30|\n",
      "| Justin, 19|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.text('file:///usr/local/spark/examples/src/main/resources/people.txt').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name;age;job\r\n",
      "Jorge;30;Developer\r\n",
      "Bob;32;Developer\r\n"
     ]
    }
   ],
   "source": [
    "! cat /usr/local/spark/examples/src/main/resources/people.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "---\n",
    "   |\n",
    "    --- part00000.csv\n",
    "    --- part00001.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('file:///usr/local/spark/examples/src/main/resources/people.csv', sep=';', header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"Michael\"}\r\n",
      "{\"name\":\"Andy\", \"age\":30}\r\n",
      "{\"name\":\"Justin\", \"age\":19}\r\n"
     ]
    }
   ],
   "source": [
    "! cat /usr/local/spark/examples/src/main/resources/people.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('file:///usr/local/spark/examples/src/main/resources/people.json').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Katowice'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.json('file:///home/jovyan/work/sample_file.json', multiLine=True).first().adres.miasto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- adres: struct (nullable = true)\n",
      " |    |-- miasto: string (nullable = true)\n",
      " |-- imię: string (nullable = true)\n",
      " |-- wiek_dzieci: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('file:///home/jovyan/work/sample_file.json', multiLine=True).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|  miasto|\n",
      "+--------+\n",
      "|Katowice|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.json('file:///home/jovyan/work/sample_file.json', multiLine=True) \\\n",
    "    .select('adres.miasto').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj\u0001\u0004\u0016avro.schema�\u0003{\"type\": \"record\", \"namespace\": \"example.avro\", \"name\": \"User\", \"fields\": [{\"type\": \"string\", \"name\": \"name\"}, {\"type\": [\"string\", \"null\"], \"name\": \"favorite_color\"}, {\"type\": {\"items\": \"int\", \"type\": \"array\"}, \"name\": \"favorite_numbers\"}]}\u0014avro.codec\bnull\u0000n�~��B;{���/�л\u00040\f",
      "Alyssa\u0002\b\u0006\u0012\u001e",
      "(\u0000\u0006Ben\u0000\u0006red\u0000n�~��B;{���/�л"
     ]
    }
   ],
   "source": [
    "! cat /usr/local/spark/examples/src/main/resources/users.avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"avro\").load('file:///usr/local/spark/examples/src/main/resources/users.avro').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORC\u0011\u0000\u0000\r\n",
      "\u0006\u0012\u0004\b\u0002P\u0000A\u0000\u0000\r\n",
      "\u001e",
      "\r\n",
      "\u0005\u0000\u0000\u0000\u0000\u0000\u0012\u0015\b\u0002\"\u000f\r\n",
      "\u0006Alyssa\u0012\u0003Ben\u0018\u0012P\u0000<\u0000\u0000!\u0010\r\n",
      "\u001f\r\n",
      "\t\u0000\u0011\u0001L\u0012\u0012\b\u0001\"\f",
      "\r\n",
      "\u0003red\u0012\u0003red\u0018\u0006P\u0001\u001b\u0000\u0000\r\n",
      "\u000b",
      "\r\n",
      "\u0003\u0000\u0000\u0000\u0012\u0004\b\u0002P\u0000+\u0000\u0000\r\n",
      "\u0013\r\n",
      "\u0003\u0000\u0000\u0000\u0012\f",
      "\b\u0004\u0012\u0006\b\u0006\u0010(\u0018^P\u0000\u0013\u0000\u0000AlyssaBen\u0007\u0000\u0000F\u0001c\u0005\u0000\u0000�@\u0007\u0000\u0000red\u0007\u0000\u0000B\u0000�\u0007\u0000\u0000F\u0001@\u000b",
      "\u0000\u0000�\u0003\u0006\f",
      "e�\u0000\u0000t\u001c",
      "\r\n",
      "\u0006\b\u0006\u0010\u0000\u0018\u000b",
      "\u0005\b\b\u0001\u0018#\u0005\b\b\u0002\u0018!\u0005\b\b\u0003\u0018\u0010\u0005\bh\u0004\u0018\u0018\r\n",
      "\u0006\b\u0001\u0010\u0001\u0018\f",
      "\r\n",
      "\u0006\b\u0002\u0010\u0001\u0018\u0006\r\n",
      "\u0006\b\u0000\u0010\u0002\u0018\u0005\u0005\u0018\u0000\u0002\u0005\u0010\u0000\u0002\u0015\b\u0000\u0003\u0005\u0010`\u0001\u0010\u0004\u0018\b\u0012\u0002\b\u0000\u0012\u0002\b\u0002\u0012\u0002\b\u0002\u0012\u0002\b\u0002\u0012\u0002\b\u0002�\u0000\u0000\r\n",
      "M\r\n",
      "\u0004\b\u0002P\u0000\r\n",
      "\u0017\b\u0002\"\u000f\r\n",
      "\u0006Alyssa\u0012\u0003Ben\u0018\u0012P\u0000X\u0012\r\n",
      "\u0014\b\u0001\"\f",
      "\r\n",
      "\u0003red\u0012\u0003red\u0018\u0006P\u0001X\u0011\r\n",
      "\u0006\b\u0002P\u0000X\u0006\r\n",
      "\u000e\b\u0004\u0012\u0006\b\u0006\u0010(\u0018^P\u0000X\bJ\u0001\u0000�\u0001�\b\u0003\u0010�\u0002\u001a\r\n",
      "\b\u0003\u0010w\u00181 e(\u0002\"/\b\f",
      "\u0012\u0003\u0001\u0002\u0003\u001a\u0004name\u001a\u000efavorite_color\u001a\u0010f\u0011\u0010(numbers\"\u0002\b\u0007\u0005\u0004�B\u0005\b\r\n",
      "\u0012\u0001\u0004\"\u0002\b\u00030\u0002:\u0004\b\u0002P\u0000:\u0017\b\u0002\"\u000f\r\n",
      "\u0006Alyssa\u0012\u0003Ben\u0018\u0012P\u0000X\u0012:\u0014\b\u0001\"\f",
      "\r\n",
      "\u0003red\u0012\u0003red\u0018\u0006P\u0001X\u0011:\u0006\u00015XX\u0006:\u000e\b\u0004\u0012\u0006\b\u0006\u0010(\u0018^P\u0000X\b@�NH\u0000\b�\u0001\u0010\u0002\u0018��\u0010\"\u0002\u0000\f",
      "(R0\u0006��\u0003\u0003ORC\u0018"
     ]
    }
   ],
   "source": [
    "! cat /usr/local/spark/examples/src/main/resources/users.orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format('orc').load('file:///usr/local/spark/examples/src/main/resources/users.orc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\u0002\u0000\u0000\u0000\u0003\u0002\u0003\u0000\u0000\u0000red\u0015\u0000\u00158\u00158,\u0015\r\n",
      "\u0015\u0000\u0015\u0006\u0015\u0006\u0000\u0000\u001c",
      "\u0014\u0002\u0000\u0000\u0000\u0003\u000e\u0005\u0006@\u000f\u0003\u0000\u0000\u0000\t\u0000\u0000\u0000\u000f\u0000\u0000\u0000\u0014\u0000\u0000\u0000\u0015\u0002\u0019\\H\u0011example.avro.User\u0015\u0006\u0000\u0015\f",
      "%\u0000\u0018\u0004name%\u0000\u0000\u0015\f",
      "%\u0002\u0018\u000efavorite_color%\u0000\u00005\u0000\u0018\u0010favorite_numbers\u0015\u0002\u0015\u0006\u0000\u0015\u0002%\u0004\u0018\u0005array\u0000\u0016\u0004\u0019\u001c",
      "\u0019<&\b\u001c",
      "\u0015\f",
      "\u0019%\b\u0000\u0019\u0018\u0004name\u0015\u0002\u0016\u0004\u0016D\u0016H&\b\u0000\u0000&P\u001c",
      "\u0015\f",
      "\u00195\b\u0000\u0006\u0019\u0018\u000efavorite_color\u0015\u0002\u0016\u0004\u0016<\u0016@&P\u0000\u0000&�\u0001\u001c",
      "\u0015\u0002\u0019%\u0000\u0006\u0019(\u0010favorite_numbers\u0005array\u0015\u0002\u0016\r\n",
      "\u0016Z\u0016Z&�\u0001\u0000\u0000\u0016�\u0001\u0016\u0004\u0000\u0019\u001c",
      "\u0018\u000b",
      "avro.schema\u0018�\u0001{\"type\":\"record\",\"name\":\"User\",\"namespace\":\"example.avro\",\"fields\":[{\"name\":\"name\",\"type\":\"string\"},{\"name\":\"favorite_color\",\"type\":[\"string\",\"null\"]},{\"name\":\"favorite_numbers\",\"type\":{\"type\":\"array\",\"items\":\"int\"}}]}\u0000\u0018\u0018parquet-mr version 1.4.3\u0000�\u0001\u0000\u0000PAR1"
     ]
    }
   ],
   "source": [
    "! cat /usr/local/spark/examples/src/main/resources/users.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('file:///usr/local/spark/examples/src/main/resources/users.parquet').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JDBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://some-mysql\")  \\\n",
    "    .option(\"dbtable\", 'db.apps_countries') \\\n",
    "    .option(\"user\", 'root') \\\n",
    "    .option(\"password\", 'my-secret-pw') \\\n",
    "    .option('driver', 'com.mysql.cj.jdbc.Driver') \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "substring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------------------+------------+\n",
      "| id|country_code|       country_name|first_letter|\n",
      "+---+------------+-------------------+------------+\n",
      "|  1|          AF|        Afghanistan|           A|\n",
      "|  2|          AL|            Albania|           A|\n",
      "|  3|          DZ|            Algeria|           A|\n",
      "|  4|          DS|     American Samoa|           A|\n",
      "|  5|          AD|            Andorra|           A|\n",
      "|  6|          AO|             Angola|           A|\n",
      "|  7|          AI|           Anguilla|           A|\n",
      "|  8|          AQ|         Antarctica|           A|\n",
      "|  9|          AG|Antigua and Barbuda|           A|\n",
      "| 10|          AR|          Argentina|           A|\n",
      "| 11|          AM|            Armenia|           A|\n",
      "| 12|          AW|              Aruba|           A|\n",
      "| 13|          AU|          Australia|           A|\n",
      "| 14|          AT|            Austria|           A|\n",
      "| 15|          AZ|         Azerbaijan|           A|\n",
      "| 16|          BS|            Bahamas|           B|\n",
      "| 17|          BH|            Bahrain|           B|\n",
      "| 18|          BD|         Bangladesh|           B|\n",
      "| 19|          BB|           Barbados|           B|\n",
      "| 20|          BY|            Belarus|           B|\n",
      "+---+------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reader.withColumn('first_letter', substring('country_name', 0, 1)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = reader.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------------------+\n",
      "| id|country_code|       country_name|\n",
      "+---+------------+-------------------+\n",
      "|  1|          AF|        Afghanistan|\n",
      "|  2|          AL|            Albania|\n",
      "|  3|          DZ|            Algeria|\n",
      "|  4|          DS|     American Samoa|\n",
      "|  5|          AD|            Andorra|\n",
      "|  6|          AO|             Angola|\n",
      "|  7|          AI|           Anguilla|\n",
      "|  8|          AQ|         Antarctica|\n",
      "|  9|          AG|Antigua and Barbuda|\n",
      "| 10|          AR|          Argentina|\n",
      "| 11|          AM|            Armenia|\n",
      "| 12|          AW|              Aruba|\n",
      "| 13|          AU|          Australia|\n",
      "| 14|          AT|            Austria|\n",
      "| 15|          AZ|         Azerbaijan|\n",
      "| 16|          BS|            Bahamas|\n",
      "| 17|          BH|            Bahrain|\n",
      "| 18|          BD|         Bangladesh|\n",
      "| 19|          BB|           Barbados|\n",
      "| 20|          BY|            Belarus|\n",
      "+---+------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reader.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "badges = spark.table('badges').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   Id|\n",
      "+-----+\n",
      "|82946|\n",
      "|82947|\n",
      "|82949|\n",
      "|82950|\n",
      "|82951|\n",
      "|82952|\n",
      "|82953|\n",
      "|82954|\n",
      "|82955|\n",
      "|82956|\n",
      "|82957|\n",
      "|82958|\n",
      "|82959|\n",
      "|82960|\n",
      "|82961|\n",
      "|82962|\n",
      "|82963|\n",
      "|82964|\n",
      "|82965|\n",
      "|82966|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "badges.select('Id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "badges2 = spark.table('badges').select('Id').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|   Id|\n",
      "+-----+\n",
      "|82946|\n",
      "|82947|\n",
      "|82949|\n",
      "|82950|\n",
      "|82951|\n",
      "|82952|\n",
      "|82953|\n",
      "|82954|\n",
      "|82955|\n",
      "|82956|\n",
      "|82957|\n",
      "|82958|\n",
      "|82959|\n",
      "|82960|\n",
      "|82961|\n",
      "|82962|\n",
      "|82963|\n",
      "|82964|\n",
      "|82965|\n",
      "|82966|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "badges2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, UserId: int, Name: string, Date: timestamp, Class: int, TagBased: boolean]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badges.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.table('badges').select('Id').unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from ES\n",
    "\n",
    "```\n",
    "es_index = spark.read.format('......es.loader').load('index-2019.09.25')\n",
    "es_index.where(col('col1') == 4) # ---> transformed to es:9200/index-2019-09-25/_search?query=col1:4\n",
    "```\n",
    "\n",
    "https://github.com/elastic/elasticsearch-hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges with CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[imie: string, nazwisko: string, miasto: string, lubi_sparka: string, wiek: string]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('file:///home/jovyan/work/sample.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[imie: string, nazwisko: string, miasto: string, lubi_sparka: string, wiek: string, data_urodzenia: string, ulubione_liczby: string]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('file:///home/jovyan/work/sample.csv', header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading own Avro file\n",
    "\n",
    "Check if the file created in pure python is readable by spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, UserId: int, Name: string, Date: timestamp, Class: string, TagBased: boolean]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.format('avro').load('file:///home/jovyan/work/data.avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 650 Sep 25 09:12 data.avro\r\n"
     ]
    }
   ],
   "source": [
    "!ls -la data.avro"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
